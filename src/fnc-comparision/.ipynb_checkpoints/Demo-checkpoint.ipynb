{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from sklearn import feature_extraction\n",
    "import math\n",
    "from features import word2vec_features, createWord2VecDict, sentiment_features\n",
    "from features import word_overlap_features, refuting_features, polarity_features\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from score import report_score, LABELS, score_submission\n",
    "from deepModel import fnc_score, load_split_vectors, vectorsExist, implementDeepModel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method\n",
    "\n",
    "def generate_features(headline, body):\n",
    "    X_overlap = word_overlap_features(headline, body)\n",
    "    print('word overlap fets generated...')\n",
    "    X_refuting = refuting_features(headline, body)\n",
    "    print('refuting fets generated...')\n",
    "    X_polarity = polarity_features(headline, body)\n",
    "    print('polarity fets generated...')\n",
    "    X = np.c_[X_polarity, X_refuting, X_overlap]\n",
    "    return X\n",
    "\n",
    "def preprocess(stances, bodies):\n",
    "    processed_heads, processed_bodies = [], []\n",
    "    \n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric, stop words.\n",
    "    for headline in stances:\n",
    "        clean_head = \" \".join(re.findall(r'\\w+', headline, flags=re.UNICODE)).lower()\n",
    "        tok_head = [t for t in nltk.word_tokenize(clean_head)]\n",
    "        pro_head = [w for w in tok_head if w not in feature_extraction.text.ENGLISH_STOP_WORDS and len(w) > 1 and (len(w)!=2 or w[0]!=w[1]) ]\n",
    "        processed_heads.append(' '.join(pro_head))\n",
    "        \n",
    "    for body in bodies:\n",
    "        clean_body = \" \".join(re.findall(r'\\w+', body, flags=re.UNICODE)).lower()\n",
    "        tok_body = [t for t in nltk.word_tokenize(clean_body)]\n",
    "        pro_body = [w for w in tok_body if w not in feature_extraction.text.ENGLISH_STOP_WORDS and len(w) > 1 and (len(w)!=2 or w[0]!=w[1]) ]\n",
    "        processed_bodies.append(' '.join(pro_body))\n",
    "    \n",
    "    return processed_heads, processed_bodies\n",
    "\n",
    "def cross_validation_split(dataset, folds=3):\n",
    "    dataset_split = []\n",
    "    dataset_copy = dataset.tolist()\n",
    "    fold_size = math.ceil(int(len(dataset) / folds))\n",
    "    for i in range(folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            fold.append(dataset_copy.pop())\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "    \n",
    "def retrieveHeadBody(dataset_type, isSmall):\n",
    "    l = os.getcwd().split('/')\n",
    "    l.pop()\n",
    "    l.pop()\n",
    "    # if isSmall and dataset_type == 'train':\n",
    "    #     file_head = '/'.join(l) + \"/input_data/smallData.csv\"\n",
    "    # else:\n",
    "    file_head = '/'.join(l) + \"/input_data/\"+dataset_type+\"_stances.csv\"    \n",
    "    file_body = '/'.join(l) + \"/input_data/\"+dataset_type+\"_bodies.csv\"\n",
    "    head = pd.read_csv(file_head)\n",
    "    body = pd.read_csv(file_body)\n",
    "    head_array = head.values\n",
    "    body_array = body.values\n",
    "    labels = head_array[:,2]\n",
    "    stance_ids = head_array[:,1]\n",
    "    body_ids = body_array[:,0]\n",
    "    new_lab = []\n",
    "    for i in labels:\n",
    "        if i == 'unrelated':\n",
    "            new_lab.append(3)\n",
    "        if i == 'agree':\n",
    "            new_lab.append(0)\n",
    "        if i == 'discuss':\n",
    "            new_lab.append(2)\n",
    "        if i == 'disagree':\n",
    "            new_lab.append(1)\n",
    "    \n",
    "    pHead, pBody = preprocess(head_array[:,0], body_array[:,1])\n",
    "    return pHead, pBody, stance_ids, body_ids, new_lab\n",
    "\n",
    "def prepare_train_data(isSmall):\n",
    "    pHead, pBody, stance_ids, body_ids, new_lab = retrieveHeadBody('train', isSmall)\n",
    "    trainHead, valHead, trainLab, valLab, idTrain, idVal = train_test_split(pHead, new_lab, stance_ids, test_size=0.20, random_state=42)\n",
    "\n",
    "    \n",
    "    valBody = []\n",
    "    for fid in idVal:\n",
    "        valBody.append(pBody[body_ids.tolist().index(fid)])\n",
    "        \n",
    "    trainBody = []\n",
    "    for fid in idTrain:\n",
    "        trainBody.append(pBody[body_ids.tolist().index(fid)])\n",
    "    \n",
    "    #tpHead, tpBody, tstance_ids, tbody_ids, tnew_lab = retrieveHeadBody('competition_test',isSmall)\n",
    "    #createWord2VecDict(pHead, pBody, tpHead, tpBody)\n",
    "    return trainHead, trainBody, trainLab, valHead, valBody, valLab\n",
    "\n",
    "def prepare_test_data(isSmall):\n",
    "    pHead, pBody, stance_ids, body_ids, new_lab = retrieveHeadBody('competition_test',isSmall)\n",
    "    testBody = []\n",
    "    for fid in stance_ids:\n",
    "        testBody.append(pBody[body_ids.tolist().index(fid)])\n",
    "    \n",
    "    return pHead, testBody, new_lab\n",
    "\n",
    "def execGradientBoosting(isSmall):\n",
    "    # if vectorsExist() and not isSmall:\n",
    "    #     trainSentiment_feats, valSentiment_feats, testSentiment_feats, train_wvFeats, val_wvFeats, test_wvFeats, trainBase_feats, valBase_feats, testBase_feats, trainLabels, valLabels, testLabels = load_split_vectors()\n",
    "    # else:\n",
    "        #tHeadLine, tBody, tLabels, vHeadLine, vBody, vLabels = prepare_data_folds()\n",
    "        \n",
    "    trainHeadLine, trainBody, trainLabels, valHeadLine, valBody, valLabels = prepare_train_data(isSmall)\n",
    "    trainLabels = np.reshape(trainLabels,(len(trainLabels),1))\n",
    "    valLabels = np.reshape(valLabels,(len(valLabels),1))\n",
    "    \n",
    "    print('Data prepared and loaded')\n",
    "    trainSentiment_feats = sentiment_features(trainHeadLine, trainBody)\n",
    "    print('Train sentiment features generated....')\n",
    "    trainBase_feats = generate_features(trainHeadLine, trainBody)\n",
    "    print('Train baseline features generated....')\n",
    "    trainHead_wvfeats, trainBody_wvfeats = word2vec_features(trainHeadLine, trainBody)\n",
    "    train_wvFeats = []\n",
    "    for x in range(len(trainHead_wvfeats)):\n",
    "        train_wvFeats.append(np.concatenate((trainHead_wvfeats[x], trainBody_wvfeats[x])))\n",
    "    train_wvFeats = np.array(train_wvFeats)\n",
    "    print('Train word2vec features generated....')\n",
    "    \n",
    "    \n",
    "    valBase_feats = generate_features(valHeadLine, valBody)\n",
    "    print('Validation baseline features generated....')\n",
    "    valSentiment_feats = sentiment_features(valHeadLine, valBody)\n",
    "    print('Validation sentiment features generated....')\n",
    "    valHead_wvfeats, valBody_wvfeats = word2vec_features(valHeadLine, valBody)\n",
    "    val_wvFeats = []\n",
    "    for x in range(len(valHead_wvfeats)):\n",
    "        val_wvFeats.append(np.concatenate((valHead_wvfeats[x], valBody_wvfeats[x])))\n",
    "    val_wvFeats = np.array(val_wvFeats)\n",
    "    print('Validation word2vec features generated....')\n",
    "    \n",
    "    testHeadLine, testBody, testLabels = prepare_test_data(isSmall)\n",
    "    testHead_wvfeats, testBody_wvfeats = word2vec_features(testHeadLine, testBody)\n",
    "    print('Test word2vec features generated....')\n",
    "    testSentiment_feats = sentiment_features(testHeadLine, testBody)\n",
    "    print('Test sentiment features generated....')\n",
    "    testBase_feats = generate_features(testHeadLine, testBody)\n",
    "    print('Test baseline features generated....')\n",
    "    test_wvFeats = []\n",
    "    for x in range(len(testHead_wvfeats)):\n",
    "        test_wvFeats.append(np.concatenate((testHead_wvfeats[x], testBody_wvfeats[x])))\n",
    "        \n",
    "    \n",
    "    \n",
    "    train_X = np.hstack((train_wvFeats, trainSentiment_feats))\n",
    "    train_X = np.hstack((train_X, trainBase_feats))\n",
    "    val_X = np.hstack((val_wvFeats, valSentiment_feats))\n",
    "    val_X = np.hstack((val_X, valBase_feats))\n",
    "    \n",
    "    test_X = np.hstack((test_wvFeats, testSentiment_feats))\n",
    "    test_X = np.hstack((test_X, testBase_feats))\n",
    "    \n",
    "    clf = GradientBoostingClassifier(n_estimators=5, random_state=14128, verbose=True)\n",
    "    clf.fit(train_X, trainLabels)\n",
    "        \n",
    "    test_pred = clf.predict_proba(test_X)\n",
    "    test_p = np.argmax(test_pred, axis=1)\n",
    "    testPredictions = [round(value) for value in test_p]\n",
    "    val_pred = clf.predict_proba(val_X)\n",
    "    val_p = np.argmax(val_pred, axis=1)\n",
    "    valPredictions = [round(value) for value in val_p]\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(valLabels, valPredictions)\n",
    "    print(\"Accuracy over validation dataset: %.2f%%\" % (accuracy * 100.0))\n",
    "    print('Score over validation dataset set: ',fnc_score(valLabels, valPredictions))\n",
    "    \n",
    "    y_pred = pd.Series(valPredictions)\n",
    "    valLabels = np.array(valLabels)\n",
    "    valLabels = valLabels.reshape(valLabels.shape[0])\n",
    "    y_true = pd.Series(valLabels)\n",
    "    print('Confusion matrix over validation dataset: ')\n",
    "    print(pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(testLabels, testPredictions)\n",
    "    print(\"Accuracy over test dataset: %.2f%%\" % (accuracy * 100.0))\n",
    "    print('Score over test dataset set: ',fnc_score(testLabels, testPredictions))\n",
    "    \n",
    "    y_pred = pd.Series(testPredictions)\n",
    "    testLabels = np.array(testLabels)\n",
    "    testLabels = testLabels.reshape(testLabels.shape[0])\n",
    "    y_true = pd.Series(testLabels)\n",
    "    print('Confusion matrix over test data: ')\n",
    "    print(pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    pred_path = '/../../output_data/treePred.pickle'\n",
    "    pickle_out = open(pred_path,\"wb\")\n",
    "    pickle.dump(test_pred, pickle_out)\n",
    "    pickle_out.close()\n",
    "    implementDeepModel(trainSentiment_feats, valSentiment_feats, testSentiment_feats, train_wvFeats, val_wvFeats, test_wvFeats, trainBase_feats, valBase_feats, testBase_feats, trainLabels, valLabels, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared and loaded\n",
      "Train sentiment features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Train baseline features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Train word2vec features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Validation baseline features generated....\n",
      "Validation sentiment features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Validation word2vec features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Test word2vec features generated....\n",
      "Test sentiment features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Test baseline features generated....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1       27238.4524            1.31m\n",
      "         2       24250.3577           58.86s\n",
      "         3       22055.5658           39.26s\n",
      "         4       20337.5138           19.65s\n",
      "         5       18956.3492            0.00s\n",
      "Accuracy over validation dataset: 82.35%\n",
      "Score over validation dataset set:  62.93865747177576\n",
      "Confusion matrix over validation dataset: \n",
      "Predicted   1     2     3   All\n",
      "True                           \n",
      "0           3   182   518   703\n",
      "1          15    45   120   180\n",
      "2           0   937   842  1779\n",
      "3           0    54  7279  7333\n",
      "All        18  1218  8759  9995\n",
      "Accuracy over test dataset: 75.96%\n",
      "Score over test dataset set:  50.53749597682652\n",
      "Confusion matrix over test data: \n",
      "Predicted    1     2      3    All\n",
      "True                              \n",
      "0            2   259   1642   1903\n",
      "1            8    52    637    697\n",
      "2            7  1302   3155   4464\n",
      "3          266    90  17993  18349\n",
      "All        283  1703  23427  25413\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/../../output_data/treePred.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bea2273dc7d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexecGradientBoosting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-79483922487b>\u001b[0m in \u001b[0;36mexecGradientBoosting\u001b[0;34m(isSmall)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrownames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0mpred_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/../../output_data/treePred.pickle'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mpickle_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mpickle_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/../../output_data/treePred.pickle'"
     ]
    }
   ],
   "source": [
    "execGradientBoosting(False)\n",
    "\n",
    "\n",
    "\n",
    "print('--------------------------------------------')\n",
    "print('--------------------------------------------')\n",
    "print('Removing some of the unrelated samples....')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execGradientBoosting(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared and loaded\n",
      "Train sentiment features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Train baseline features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Train word2vec features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Validation baseline features generated....\n",
      "Validation sentiment features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Validation word2vec features generated....\n"
     ]
    }
   ],
   "source": [
    "isSmall = False\n",
    "trainHeadLine, trainBody, trainLabels, valHeadLine, valBody, valLabels = prepare_train_data(isSmall)\n",
    "trainLabels = np.reshape(trainLabels,(len(trainLabels),1))\n",
    "valLabels = np.reshape(valLabels,(len(valLabels),1))\n",
    "   \n",
    "print('Data prepared and loaded')\n",
    "trainSentiment_feats = sentiment_features(trainHeadLine, trainBody)\n",
    "print('Train sentiment features generated....')\n",
    "trainBase_feats = generate_features(trainHeadLine, trainBody)\n",
    "print('Train baseline features generated....')\n",
    "trainHead_wvfeats, trainBody_wvfeats = word2vec_features(trainHeadLine, trainBody)\n",
    "train_wvFeats = []\n",
    "for x in range(len(trainHead_wvfeats)):\n",
    "    train_wvFeats.append(np.concatenate((trainHead_wvfeats[x], trainBody_wvfeats[x])))\n",
    "train_wvFeats = np.array(train_wvFeats)\n",
    "print('Train word2vec features generated....')\n",
    "   \n",
    "    \n",
    "valBase_feats = generate_features(valHeadLine, valBody)\n",
    "print('Validation baseline features generated....')\n",
    "valSentiment_feats = sentiment_features(valHeadLine, valBody)\n",
    "print('Validation sentiment features generated....')\n",
    "valHead_wvfeats, valBody_wvfeats = word2vec_features(valHeadLine, valBody)\n",
    "val_wvFeats = []\n",
    "for x in range(len(valHead_wvfeats)):\n",
    "    val_wvFeats.append(np.concatenate((valHead_wvfeats[x], valBody_wvfeats[x])))\n",
    "val_wvFeats = np.array(val_wvFeats)\n",
    "print('Validation word2vec features generated....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "testHeadLine, testBody, testLabels = prepare_test_data(isSmall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "testHeadLine = ['ferguson riots pregnant woman loses eye cops bean bag round car window']\n",
    "testBody     = ['respected senior french police officer investigating charlie hebdo magazine massacre took life mere hours horrific attacks stunned world commissioner helric fredou 45 turned gun police office limoges wednesday night reported france colleague body 1am thursday day gunmen fired satirical magazine office left 12 people dead speaking sister publication mirror online union commissioners national police confirmed mr fredou taken life statement released death union spokesman said great sadness informed morning death colleague helric fredou assigned deputy director regional service judicial police limoges particular day national mourning police commissioners hit hard tragic death union commissioners national police like present condolences relatives helric difficult times special thought colleagues mr fredou single children began career police officer 1997 working versailles eventually returned home town limoges 2012 deputy director regional police service french media reports suggest depressed suffering burnout brothers said cherif kouachi launched wednesday devastating attack office french satirical magazine attack left 12 people dead friday pair shot dead police taking man hostage family run printing press dammartin en goele separate incident friday amedy coulibaly 32 took 15 people hostage paris supermarket hostages killed incident coulibaly']\n",
    "testLabels   = [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Test word2vec features generated....\n",
      "Test sentiment features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Test baseline features generated....\n"
     ]
    }
   ],
   "source": [
    "testHead_wvfeats, testBody_wvfeats = word2vec_features(testHeadLine, testBody)\n",
    "print('Test word2vec features generated....')\n",
    "testSentiment_feats = sentiment_features(testHeadLine, testBody)\n",
    "print('Test sentiment features generated....')\n",
    "testBase_feats = generate_features(testHeadLine, testBody)\n",
    "print('Test baseline features generated....')\n",
    "test_wvFeats = []\n",
    "for x in range(len(testHead_wvfeats)):\n",
    "    test_wvFeats.append(np.concatenate((testHead_wvfeats[x], testBody_wvfeats[x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1       27238.4524            1.36m\n",
      "         2       24250.3577            1.01m\n",
      "         3       22055.5658           40.50s\n",
      "         4       20337.5138           20.28s\n",
      "         5       18956.3492            0.00s\n"
     ]
    }
   ],
   "source": [
    "train_X = np.hstack((train_wvFeats, trainSentiment_feats))\n",
    "train_X = np.hstack((train_X, trainBase_feats))\n",
    "val_X = np.hstack((val_wvFeats, valSentiment_feats))\n",
    "val_X = np.hstack((val_X, valBase_feats))\n",
    "    \n",
    "test_X = np.hstack((test_wvFeats, testSentiment_feats))\n",
    "test_X = np.hstack((test_X, testBase_feats))\n",
    "    \n",
    "clf = GradientBoostingClassifier(n_estimators=5, random_state=14128, verbose=True)\n",
    "clf.fit(train_X, trainLabels)\n",
    "        \n",
    "test_pred = clf.predict_proba(test_X)\n",
    "test_p = np.argmax(test_pred, axis=1)\n",
    "testPredictions = [round(value) for value in test_p]\n",
    "val_pred = clf.predict_proba(val_X)\n",
    "val_p = np.argmax(val_pred, axis=1)\n",
    "valPredictions = [round(value) for value in val_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "print(test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
