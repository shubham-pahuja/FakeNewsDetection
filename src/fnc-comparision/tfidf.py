# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XKkRLvW2KSaJ_8aENgtpt1QcuIWs9b6j
"""

import nltk

train_stance_url='/home/aman/Desktop/fnc/SMAI-fnc/fnc/input_data/train_stances.csv'
train_bodies_url='/home/aman/Desktop/fnc/SMAI-fnc/fnc/input_data/train_bodies.csv'

import pandas as pd
import numpy as np
stances_train = pd.read_csv(train_stance_url)
body_train = pd.read_csv(train_bodies_url)
train = pd.merge(stances_train, body_train, how='left', on='Body ID')

train[0:2]

train.shape

dic={'agree':0,'disagree':1,'discuss':2,'unrelated':3}
train['Stance']=train['Stance'].map(dic)

train[0:1]

#nltk.download('stopwords')

english_stemmer = nltk.stem.SnowballStemmer('english')
token_pattern = r"(?u)\b\w\w+\b"
stopwords = set(nltk.corpus.stopwords.words('english'))

def stem_tokens(tokens, stemmer):
    stemmed = []
    for token in tokens:
        stemmed.append(stemmer.stem(token))
    return stemmed

import re
def preprocess_data(line,
                    token_pattern=token_pattern,
                    exclude_stopword=True,
                    stem=True):
    token_pattern = re.compile(token_pattern, flags = re.UNICODE)
    tokens = [x.lower() for x in token_pattern.findall(line)]
    tokens_stemmed = tokens
    if stem:
#         pass
        tokens_stemmed = stem_tokens(tokens, english_stemmer)
    if exclude_stopword:
#         pass
        tokens_stemmed = [x for x in tokens_stemmed if x not in stopwords]

    return tokens_stemmed

preprocess_data('She is running. He ran away')
# train[0:5]

# preprocess_data(train['Headline'])

"""Concatinating headline and body strings"""

train['combined']= train['Headline'] +" "+ train['articleBody']

train['combined'][0]

train['Headline'][0]

"""**Preprocessing** the combined string = Stemming + Stop words removal

Generating **unigrams**= ['this , 'is' , 'a' ,'cat']
"""

unigram=[]
print('1')
for i in train['combined']:
  unigram.append(preprocess_data(i))
print('2')
head_unigram=[]
for i in train['Headline']:
  head_unigram.append(preprocess_data(i))
print('3')
body_unigram=[]
for i in train['articleBody']:
  body_unigram.append(preprocess_data(i))
print('4')
unigram_np = np.array(unigram)
unigram_np.shape

"""**Making strings from unigrams**"""

unigram_comb=[]
for count,i in enumerate(unigram_np):
  unigram_comb.append(' '.join(unigram_np[count]))
print('5')
unigram_comb_head=[]
for count,i in enumerate(head_unigram):
  unigram_comb_head.append(' '.join(head_unigram[count]))
print('6')
unigram_comb_body=[]
for count,i in enumerate(body_unigram):
  unigram_comb_body.append(' '.join(body_unigram[count]))

len(unigram_comb)

len(unigram_comb[0])

len(unigram)
print('7')
train['headline_unigram']=np.array(unigram_comb_head)
train['combined_unigram']=np.array(unigram_comb)
train['body_unigram']=np.array(unigram_comb_body)

del(unigram)
del(head_unigram)
del(body_unigram)
del(unigram_comb)
del(unigram_comb_head)
del(unigram_comb_body)
del(stances_train)
del(body_train)
del(unigram_np)

vocab_size=5000
from sklearn.feature_extraction.text import TfidfVectorizer
vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8, min_df=2,max_features=vocab_size)
vec.fit(train["combined_unigram"]) # Tf-idf calculated on the combined training + test set
vocabulary = vec.vocabulary_
print('8')
vecH = TfidfVectorizer(ngram_range=(1, 3), max_df=0.8, min_df=2, vocabulary=vocabulary)
xHeadlineTfidf = vecH.fit_transform(train['headline_unigram']) # use ' '.join(Headline_unigram) instead of Headline since the former is already stemmed
print ('xHeadlineTfidf.shape:')
print (xHeadlineTfidf.shape)
print('9')
len(vocabulary)

xHeadlineTfidf[0]

vecB = TfidfVectorizer(ngram_range=(1, 3), max_df=0.8, min_df=2, vocabulary=vocabulary,max_features=vocab_size)
xBodyTfidf = vecB.fit_transform(train['body_unigram'])
print ('xBodyTfidf.shape:')
print (xBodyTfidf.shape)
print('10')
train[0:1]

print(xHeadlineTfidf.shape)
print(xBodyTfidf.shape)

type(xHeadlineTfidf)

from scipy.sparse import csr_matrix
xheadline= xHeadlineTfidf.toarray()
xbody=xBodyTfidf.toarray()

print(xheadline.shape)
print("body shape")
print(xbody.shape)

x= np.hstack((xheadline,xbody))
print(x.shape)
print('11')
print(train.columns)
# train['xHeadlineTfidf']=xHeadlineTfidf
# train['xBodyTfidf']=xBodyTfidf
# train['combinedTfidf']=x

y= train['Stance']
print(x.shape)
print(y.shape)
del(train)
print('12')
from numpy import loadtxt
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# =============================================================================
# seed = 7
# test_size = 0.33
# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=seed)
# 
# 
# 
# 
# 
# import tensorflow as tf
# tf.test.gpu_device_name()
# =============================================================================
