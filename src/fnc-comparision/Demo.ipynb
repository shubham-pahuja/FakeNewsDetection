{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from sklearn import feature_extraction\n",
    "import math\n",
    "from features import word2vec_features, createWord2VecDict, sentiment_features\n",
    "from features import word_overlap_features, refuting_features, polarity_features\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from score import report_score, LABELS, score_submission\n",
    "from deepModel import fnc_score, load_split_vectors, vectorsExist, implementDeepModel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method\n",
    "\n",
    "def generate_features(headline, body):\n",
    "    X_overlap = word_overlap_features(headline, body)\n",
    "    print('word overlap fets generated...')\n",
    "    X_refuting = refuting_features(headline, body)\n",
    "    print('refuting fets generated...')\n",
    "    X_polarity = polarity_features(headline, body)\n",
    "    print('polarity fets generated...')\n",
    "    X = np.c_[X_polarity, X_refuting, X_overlap]\n",
    "    return X\n",
    "\n",
    "def preprocess(stances, bodies):\n",
    "    processed_heads, processed_bodies = [], []\n",
    "    \n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric, stop words.\n",
    "    for headline in stances:\n",
    "        clean_head = \" \".join(re.findall(r'\\w+', headline, flags=re.UNICODE)).lower()\n",
    "        tok_head = [t for t in nltk.word_tokenize(clean_head)]\n",
    "        pro_head = [w for w in tok_head if w not in feature_extraction.text.ENGLISH_STOP_WORDS and len(w) > 1 and (len(w)!=2 or w[0]!=w[1]) ]\n",
    "        processed_heads.append(' '.join(pro_head))\n",
    "        \n",
    "    for body in bodies:\n",
    "        clean_body = \" \".join(re.findall(r'\\w+', body, flags=re.UNICODE)).lower()\n",
    "        tok_body = [t for t in nltk.word_tokenize(clean_body)]\n",
    "        pro_body = [w for w in tok_body if w not in feature_extraction.text.ENGLISH_STOP_WORDS and len(w) > 1 and (len(w)!=2 or w[0]!=w[1]) ]\n",
    "        processed_bodies.append(' '.join(pro_body))\n",
    "    \n",
    "    return processed_heads, processed_bodies\n",
    "\n",
    "def cross_validation_split(dataset, folds=3):\n",
    "    dataset_split = []\n",
    "    dataset_copy = dataset.tolist()\n",
    "    fold_size = math.ceil(int(len(dataset) / folds))\n",
    "    for i in range(folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            fold.append(dataset_copy.pop())\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "    \n",
    "def retrieveHeadBody(dataset_type, isSmall):\n",
    "    l = os.getcwd().split('/')\n",
    "    l.pop()\n",
    "    l.pop()\n",
    "    # if isSmall and dataset_type == 'train':\n",
    "    #     file_head = '/'.join(l) + \"/input_data/smallData.csv\"\n",
    "    # else:\n",
    "    file_head = '/'.join(l) + \"/input_data/\"+dataset_type+\"_stances.csv\"    \n",
    "    file_body = '/'.join(l) + \"/input_data/\"+dataset_type+\"_bodies.csv\"\n",
    "    head = pd.read_csv(file_head)\n",
    "    body = pd.read_csv(file_body)\n",
    "    head_array = head.values\n",
    "    body_array = body.values\n",
    "    labels = head_array[:,2]\n",
    "    stance_ids = head_array[:,1]\n",
    "    body_ids = body_array[:,0]\n",
    "    new_lab = []\n",
    "    for i in labels:\n",
    "        if i == 'unrelated':\n",
    "            new_lab.append(3)\n",
    "        if i == 'agree':\n",
    "            new_lab.append(0)\n",
    "        if i == 'discuss':\n",
    "            new_lab.append(2)\n",
    "        if i == 'disagree':\n",
    "            new_lab.append(1)\n",
    "    \n",
    "    pHead, pBody = preprocess(head_array[:,0], body_array[:,1])\n",
    "    return pHead, pBody, stance_ids, body_ids, new_lab\n",
    "\n",
    "def prepare_train_data(isSmall):\n",
    "    pHead, pBody, stance_ids, body_ids, new_lab = retrieveHeadBody('train', isSmall)\n",
    "    trainHead, valHead, trainLab, valLab, idTrain, idVal = train_test_split(pHead, new_lab, stance_ids, test_size=0.20, random_state=42)\n",
    "\n",
    "    \n",
    "    valBody = []\n",
    "    for fid in idVal:\n",
    "        valBody.append(pBody[body_ids.tolist().index(fid)])\n",
    "        \n",
    "    trainBody = []\n",
    "    for fid in idTrain:\n",
    "        trainBody.append(pBody[body_ids.tolist().index(fid)])\n",
    "    \n",
    "    #tpHead, tpBody, tstance_ids, tbody_ids, tnew_lab = retrieveHeadBody('competition_test',isSmall)\n",
    "    #createWord2VecDict(pHead, pBody, tpHead, tpBody)\n",
    "    return trainHead, trainBody, trainLab, valHead, valBody, valLab\n",
    "\n",
    "def prepare_test_data(isSmall):\n",
    "    pHead, pBody, stance_ids, body_ids, new_lab = retrieveHeadBody('competition_test',isSmall)\n",
    "    testBody = []\n",
    "    for fid in stance_ids:\n",
    "        testBody.append(pBody[body_ids.tolist().index(fid)])\n",
    "    \n",
    "    return pHead, testBody, new_lab\n",
    "\n",
    "def print_label(res):\n",
    "    if res == 3:\n",
    "        print('agree')\n",
    "    if res == 0:\n",
    "        print('unrelated')\n",
    "    if res == 2:\n",
    "        print('discuss')\n",
    "    if res == 1:\n",
    "        print('disagree')\n",
    "\n",
    "def execGradientBoosting(isSmall):\n",
    "    # if vectorsExist() and not isSmall:\n",
    "    #     trainSentiment_feats, valSentiment_feats, testSentiment_feats, train_wvFeats, val_wvFeats, test_wvFeats, trainBase_feats, valBase_feats, testBase_feats, trainLabels, valLabels, testLabels = load_split_vectors()\n",
    "    # else:\n",
    "        #tHeadLine, tBody, tLabels, vHeadLine, vBody, vLabels = prepare_data_folds()\n",
    "        \n",
    "    trainHeadLine, trainBody, trainLabels, valHeadLine, valBody, valLabels = prepare_train_data(isSmall)\n",
    "    trainLabels = np.reshape(trainLabels,(len(trainLabels),1))\n",
    "    valLabels = np.reshape(valLabels,(len(valLabels),1))\n",
    "    \n",
    "    print('Data prepared and loaded')\n",
    "    trainSentiment_feats = sentiment_features(trainHeadLine, trainBody)\n",
    "    print('Train sentiment features generated....')\n",
    "    trainBase_feats = generate_features(trainHeadLine, trainBody)\n",
    "    print('Train baseline features generated....')\n",
    "    trainHead_wvfeats, trainBody_wvfeats = word2vec_features(trainHeadLine, trainBody)\n",
    "    train_wvFeats = []\n",
    "    for x in range(len(trainHead_wvfeats)):\n",
    "        train_wvFeats.append(np.concatenate((trainHead_wvfeats[x], trainBody_wvfeats[x])))\n",
    "    train_wvFeats = np.array(train_wvFeats)\n",
    "    print('Train word2vec features generated....')\n",
    "    \n",
    "    \n",
    "    valBase_feats = generate_features(valHeadLine, valBody)\n",
    "    print('Validation baseline features generated....')\n",
    "    valSentiment_feats = sentiment_features(valHeadLine, valBody)\n",
    "    print('Validation sentiment features generated....')\n",
    "    valHead_wvfeats, valBody_wvfeats = word2vec_features(valHeadLine, valBody)\n",
    "    val_wvFeats = []\n",
    "    for x in range(len(valHead_wvfeats)):\n",
    "        val_wvFeats.append(np.concatenate((valHead_wvfeats[x], valBody_wvfeats[x])))\n",
    "    val_wvFeats = np.array(val_wvFeats)\n",
    "    print('Validation word2vec features generated....')\n",
    "    \n",
    "    testHeadLine, testBody, testLabels = prepare_test_data(isSmall)\n",
    "    testHead_wvfeats, testBody_wvfeats = word2vec_features(testHeadLine, testBody)\n",
    "    print('Test word2vec features generated....')\n",
    "    testSentiment_feats = sentiment_features(testHeadLine, testBody)\n",
    "    print('Test sentiment features generated....')\n",
    "    testBase_feats = generate_features(testHeadLine, testBody)\n",
    "    print('Test baseline features generated....')\n",
    "    test_wvFeats = []\n",
    "    for x in range(len(testHead_wvfeats)):\n",
    "        test_wvFeats.append(np.concatenate((testHead_wvfeats[x], testBody_wvfeats[x])))\n",
    "        \n",
    "    \n",
    "    \n",
    "    train_X = np.hstack((train_wvFeats, trainSentiment_feats))\n",
    "    train_X = np.hstack((train_X, trainBase_feats))\n",
    "    val_X = np.hstack((val_wvFeats, valSentiment_feats))\n",
    "    val_X = np.hstack((val_X, valBase_feats))\n",
    "    \n",
    "    test_X = np.hstack((test_wvFeats, testSentiment_feats))\n",
    "    test_X = np.hstack((test_X, testBase_feats))\n",
    "    \n",
    "    clf = GradientBoostingClassifier(n_estimators=5, random_state=14128, verbose=True)\n",
    "    clf.fit(train_X, trainLabels)\n",
    "        \n",
    "    test_pred = clf.predict_proba(test_X)\n",
    "    test_p = np.argmax(test_pred, axis=1)\n",
    "    testPredictions = [round(value) for value in test_p]\n",
    "    val_pred = clf.predict_proba(val_X)\n",
    "    val_p = np.argmax(val_pred, axis=1)\n",
    "    valPredictions = [round(value) for value in val_p]\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(valLabels, valPredictions)\n",
    "    print(\"Accuracy over validation dataset: %.2f%%\" % (accuracy * 100.0))\n",
    "    print('Score over validation dataset set: ',fnc_score(valLabels, valPredictions))\n",
    "    \n",
    "    y_pred = pd.Series(valPredictions)\n",
    "    valLabels = np.array(valLabels)\n",
    "    valLabels = valLabels.reshape(valLabels.shape[0])\n",
    "    y_true = pd.Series(valLabels)\n",
    "    print('Confusion matrix over validation dataset: ')\n",
    "    print(pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(testLabels, testPredictions)\n",
    "    print(\"Accuracy over test dataset: %.2f%%\" % (accuracy * 100.0))\n",
    "    print('Score over test dataset set: ',fnc_score(testLabels, testPredictions))\n",
    "    \n",
    "    y_pred = pd.Series(testPredictions)\n",
    "    testLabels = np.array(testLabels)\n",
    "    testLabels = testLabels.reshape(testLabels.shape[0])\n",
    "    y_true = pd.Series(testLabels)\n",
    "    print('Confusion matrix over test data: ')\n",
    "    print(pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    implementDeepModel(trainSentiment_feats, valSentiment_feats, testSentiment_feats, train_wvFeats, val_wvFeats, test_wvFeats, trainBase_feats, valBase_feats, testBase_feats, trainLabels, valLabels, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared and loaded\n",
      "Train sentiment features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Train baseline features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Train word2vec features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Validation baseline features generated....\n",
      "Validation sentiment features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Validation word2vec features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Test word2vec features generated....\n",
      "Test sentiment features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Test baseline features generated....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1       27238.4524            1.32m\n",
      "         2       24250.3577           59.73s\n",
      "         3       22055.5658           39.71s\n",
      "         4       20337.5138           19.87s\n",
      "         5       18956.3492            0.00s\n",
      "Accuracy over validation dataset: 82.35%\n",
      "Score over validation dataset set:  62.93865747177576\n",
      "Confusion matrix over validation dataset: \n",
      "Predicted   1     2     3   All\n",
      "True                           \n",
      "0           3   182   518   703\n",
      "1          15    45   120   180\n",
      "2           0   937   842  1779\n",
      "3           0    54  7279  7333\n",
      "All        18  1218  8759  9995\n",
      "Accuracy over test dataset: 75.96%\n",
      "Score over test dataset set:  50.53749597682652\n",
      "Confusion matrix over test data: \n",
      "Predicted    1     2      3    All\n",
      "True                              \n",
      "0            2   259   1642   1903\n",
      "1            8    52    637    697\n",
      "2            7  1302   3155   4464\n",
      "3          266    90  17993  18349\n",
      "All        283  1703  23427  25413\n",
      "Train on 39977 samples, validate on 9995 samples\n",
      "Epoch 1/3\n",
      "39977/39977 [==============================] - 25s 616us/sample - loss: 0.7765 - accuracy: 0.7274 - val_loss: 0.6484 - val_accuracy: 0.7488\n",
      "Epoch 2/3\n",
      "39977/39977 [==============================] - 23s 571us/sample - loss: 0.5729 - accuracy: 0.7820 - val_loss: 0.4734 - val_accuracy: 0.8261\n",
      "Epoch 3/3\n",
      "39977/39977 [==============================] - 23s 574us/sample - loss: 0.4294 - accuracy: 0.8475 - val_loss: 0.3775 - val_accuracy: 0.8611\n",
      "Accuracy over validation set:  0.8611305652826413\n",
      "Accuracy over test set:  0.8341006571439814\n",
      "Fnc score over validation set:  72.52655580890941\n",
      "Fnc score over test set:  68.07424096127025\n",
      "Validation Set confusion matrix is: \n",
      "Predicted    0     2     3   All\n",
      "True                            \n",
      "0          299   167   237   703\n",
      "1           59    51    70   180\n",
      "2          185  1025   569  1779\n",
      "3            8    42  7283  7333\n",
      "All        551  1285  8159  9995\n",
      "Test Set confusion matrix is: \n",
      "Predicted     0  1     2      3    All\n",
      "True                                  \n",
      "0           967  0   276    660   1903\n",
      "1           178  0    80    439    697\n",
      "2           843  2  2083   1536   4464\n",
      "3            54  0   148  18147  18349\n",
      "All        2042  2  2587  20782  25413\n"
     ]
    }
   ],
   "source": [
    "execGradientBoosting(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared and loaded\n",
      "Train sentiment features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Train baseline features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Train word2vec features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Validation baseline features generated....\n",
      "Validation sentiment features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Validation word2vec features generated....\n"
     ]
    }
   ],
   "source": [
    "#Preparing Train Data\n",
    "isSmall = False\n",
    "trainHeadLine, trainBody, trainLabels, valHeadLine, valBody, valLabels = prepare_train_data(isSmall)\n",
    "trainLabels = np.reshape(trainLabels,(len(trainLabels),1))\n",
    "valLabels = np.reshape(valLabels,(len(valLabels),1))\n",
    "   \n",
    "print('Data prepared and loaded')\n",
    "trainSentiment_feats = sentiment_features(trainHeadLine, trainBody)\n",
    "print('Train sentiment features generated....')\n",
    "trainBase_feats = generate_features(trainHeadLine, trainBody)\n",
    "print('Train baseline features generated....')\n",
    "trainHead_wvfeats, trainBody_wvfeats = word2vec_features(trainHeadLine, trainBody)\n",
    "train_wvFeats = []\n",
    "for x in range(len(trainHead_wvfeats)):\n",
    "    train_wvFeats.append(np.concatenate((trainHead_wvfeats[x], trainBody_wvfeats[x])))\n",
    "train_wvFeats = np.array(train_wvFeats)\n",
    "print('Train word2vec features generated....')\n",
    "   \n",
    "    \n",
    "valBase_feats = generate_features(valHeadLine, valBody)\n",
    "print('Validation baseline features generated....')\n",
    "valSentiment_feats = sentiment_features(valHeadLine, valBody)\n",
    "print('Validation sentiment features generated....')\n",
    "valHead_wvfeats, valBody_wvfeats = word2vec_features(valHeadLine, valBody)\n",
    "val_wvFeats = []\n",
    "for x in range(len(valHead_wvfeats)):\n",
    "    val_wvFeats.append(np.concatenate((valHead_wvfeats[x], valBody_wvfeats[x])))\n",
    "val_wvFeats = np.array(val_wvFeats)\n",
    "print('Validation word2vec features generated....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in trainLabels:\n",
    "    if i == 0:\n",
    "        print(c)\n",
    "        break\n",
    "    c = c + 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "video marine saved helmet taliban sniper attack\n",
      "importance wearing helmet field battle perfectly illustrated video uploaded youtube marine video marines seen building search source gunfire near doorway single file bullet suddenly bounces kevlar helmet middle soldier men quickly retreat building inspect damage soldier helmet ascertaining marine unharmed regroup consider options afghanistan veteran sam arnold says video shot conducting joint helicopter raid zad district helmand province 2013 footage uploaded youtube month received just 200 000 views marines moments shot helmet videos telegraph men try happened man asked 200 women sleep terrifying headcam shows cyclist land feet hit car greatest speech sports coach trousers hands children baffled walkman cassette players danny macaskill releases breathtaking isle skye tricks video 10 amazing tips improve sleep\n"
     ]
    }
   ],
   "source": [
    "print(trainLabels[8])\n",
    "print(trainHeadLine[8])\n",
    "print(trainBody[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1       27238.4524            1.56m\n",
      "         2       24250.3577            1.16m\n",
      "         3       22055.5658           46.60s\n",
      "         4       20337.5138           23.26s\n",
      "         5       18956.3492            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=5,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=14128, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=True,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training\n",
    "train_X = np.hstack((train_wvFeats, trainSentiment_feats))\n",
    "train_X = np.hstack((train_X, trainBase_feats))\n",
    "val_X = np.hstack((val_wvFeats, valSentiment_feats))\n",
    "val_X = np.hstack((val_X, valBase_feats))\n",
    "clf = GradientBoostingClassifier(n_estimators=5, random_state=14128, verbose=True)\n",
    "clf.fit(train_X, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#News In Indian Language\n",
    "Language = 'hi'\n",
    "HeadLine = 'लॉकडाउन में ऑड-ईवन की तरह खुलें बाजार, दिल्ली सरकार ने केंद्र को भेजा प्रस्ताव'\n",
    "Body     = 'कोरोना वायरस संकट के बढ़ते मामलों के बीच दिल्ली सरकार की ओर से केंद्र सरकार को लॉकडाउन के लिए प्रस्ताव भेज दिया गया है. दिल्ली के स्वास्थ्य मंत्री सत्येंद्र जैन ने शुक्रवार कोरोना वायरस संकट को लेकर मीडिया से बात की. उन्होंने इस दौरान कहा कि दिल्ली सरकार की ओर से केंद्र सरकार को लॉकडाउन के लिए सुझाव भेजा जा चुका है. हम चाहते हैं कि बसों और मेट्रो को लिमिटेड तरीके से खोला जाए.सत्येंद्र जैन ने कहा कि हमने सरकार से पचास फीसदी तक मॉल खोलने को कहा है और ऑड-ईवन की तरह खुलें बाजार. इसके अलावा पब्लिक प्लेस में मास्क पहनना जरूरी कर देना चाहिए.'\n",
    "Actual_Lable = 'agree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline In English :  Open market like the odd-even in lockdown, offer government sent center\n",
      "NewsBody In English :  Corona virus from the central government on behalf of the government between the incidence of the crisis has sent the proposal to the lockdown. Health Minister Satyendra Jain Delhi on Friday to talk to the media the corona virus crisis. He has been sent suggestions during that lockdown central government on behalf of the government. We want buses and subway opened to limited ways Jaa.satyendra Jain said that we have asked to open the mall to the government to fifty percent and open market like the odd-even. Also should be required to wear masks in public place.\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "srcLang = 'hi'\n",
    "destLang   = 'en'\n",
    "Eng_Headline = translator.translate(HeadLine, dest=destLang , src=srcLang).text\n",
    "Eng_Body     = translator.translate(Body, dest=destLang , src=srcLang).text\n",
    "print('Headline In English : ',Eng_Headline)\n",
    "print('NewsBody In English : ',Eng_Body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "testHeadLine = [Eng_Headline]\n",
    "testBody     = [Eng_Body]\n",
    "testLabels   = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Test word2vec features generated....\n",
      "Test sentiment features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Test baseline features generated....\n"
     ]
    }
   ],
   "source": [
    "#Preparing Test Data\n",
    "#testHeadLine, testBody, testLabels = prepare_test_data(isSmall)\n",
    "testHead_wvfeats, testBody_wvfeats = word2vec_features(testHeadLine, testBody)\n",
    "print('Test word2vec features generated....')\n",
    "testSentiment_feats = sentiment_features(testHeadLine, testBody)\n",
    "print('Test sentiment features generated....')\n",
    "testBase_feats = generate_features(testHeadLine, testBody)\n",
    "print('Test baseline features generated....')\n",
    "test_wvFeats = []\n",
    "for x in range(len(testHead_wvfeats)):\n",
    "    test_wvFeats.append(np.concatenate((testHead_wvfeats[x], testBody_wvfeats[x])))\n",
    "    \n",
    "test_X = np.hstack((test_wvFeats, testSentiment_feats))\n",
    "test_X = np.hstack((test_X, testBase_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "test_pred = clf.predict_proba(test_X)\n",
    "test_p = np.argmax(test_pred, axis=1)\n",
    "testPredictions = [round(value) for value in test_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agree\n"
     ]
    }
   ],
   "source": [
    "res = testPredictions[0]\n",
    "print_label(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
