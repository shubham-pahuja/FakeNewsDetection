{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from sklearn import feature_extraction\n",
    "import math\n",
    "from features import word2vec_features, createWord2VecDict, sentiment_features\n",
    "from features import word_overlap_features, refuting_features, polarity_features\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from score import report_score, LABELS, score_submission\n",
    "from deepModel import fnc_score, load_split_vectors, vectorsExist, implementDeepModel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method\n",
    "\n",
    "def generate_features(headline, body):\n",
    "    X_overlap = word_overlap_features(headline, body)\n",
    "    print('word overlap fets generated...')\n",
    "    X_refuting = refuting_features(headline, body)\n",
    "    print('refuting fets generated...')\n",
    "    X_polarity = polarity_features(headline, body)\n",
    "    print('polarity fets generated...')\n",
    "    X = np.c_[X_polarity, X_refuting, X_overlap]\n",
    "    return X\n",
    "\n",
    "def preprocess(stances, bodies):\n",
    "    processed_heads, processed_bodies = [], []\n",
    "    \n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric, stop words.\n",
    "    for headline in stances:\n",
    "        clean_head = \" \".join(re.findall(r'\\w+', headline, flags=re.UNICODE)).lower()\n",
    "        tok_head = [t for t in nltk.word_tokenize(clean_head)]\n",
    "        pro_head = [w for w in tok_head if w not in feature_extraction.text.ENGLISH_STOP_WORDS and len(w) > 1 and (len(w)!=2 or w[0]!=w[1]) ]\n",
    "        processed_heads.append(' '.join(pro_head))\n",
    "        \n",
    "    for body in bodies:\n",
    "        clean_body = \" \".join(re.findall(r'\\w+', body, flags=re.UNICODE)).lower()\n",
    "        tok_body = [t for t in nltk.word_tokenize(clean_body)]\n",
    "        pro_body = [w for w in tok_body if w not in feature_extraction.text.ENGLISH_STOP_WORDS and len(w) > 1 and (len(w)!=2 or w[0]!=w[1]) ]\n",
    "        processed_bodies.append(' '.join(pro_body))\n",
    "    \n",
    "    return processed_heads, processed_bodies\n",
    "\n",
    "def cross_validation_split(dataset, folds=3):\n",
    "    dataset_split = []\n",
    "    dataset_copy = dataset.tolist()\n",
    "    fold_size = math.ceil(int(len(dataset) / folds))\n",
    "    for i in range(folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            fold.append(dataset_copy.pop())\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "    \n",
    "def retrieveHeadBody(dataset_type, isSmall):\n",
    "    l = os.getcwd().split('/')\n",
    "    l.pop()\n",
    "    l.pop()\n",
    "    # if isSmall and dataset_type == 'train':\n",
    "    #     file_head = '/'.join(l) + \"/input_data/smallData.csv\"\n",
    "    # else:\n",
    "    file_head = '/'.join(l) + \"/input_data/\"+dataset_type+\"_stances.csv\"    \n",
    "    file_body = '/'.join(l) + \"/input_data/\"+dataset_type+\"_bodies.csv\"\n",
    "    head = pd.read_csv(file_head)\n",
    "    body = pd.read_csv(file_body)\n",
    "    head_array = head.values\n",
    "    body_array = body.values\n",
    "    labels = head_array[:,2]\n",
    "    stance_ids = head_array[:,1]\n",
    "    body_ids = body_array[:,0]\n",
    "    new_lab = []\n",
    "    for i in labels:\n",
    "        if i == 'unrelated':\n",
    "            new_lab.append(3)\n",
    "        if i == 'agree':\n",
    "            new_lab.append(0)\n",
    "        if i == 'discuss':\n",
    "            new_lab.append(2)\n",
    "        if i == 'disagree':\n",
    "            new_lab.append(1)\n",
    "    \n",
    "    pHead, pBody = preprocess(head_array[:,0], body_array[:,1])\n",
    "    return pHead, pBody, stance_ids, body_ids, new_lab\n",
    "\n",
    "def prepare_train_data(isSmall):\n",
    "    pHead, pBody, stance_ids, body_ids, new_lab = retrieveHeadBody('train', isSmall)\n",
    "    trainHead, valHead, trainLab, valLab, idTrain, idVal = train_test_split(pHead, new_lab, stance_ids, test_size=0.20, random_state=42)\n",
    "\n",
    "    \n",
    "    valBody = []\n",
    "    for fid in idVal:\n",
    "        valBody.append(pBody[body_ids.tolist().index(fid)])\n",
    "        \n",
    "    trainBody = []\n",
    "    for fid in idTrain:\n",
    "        trainBody.append(pBody[body_ids.tolist().index(fid)])\n",
    "    \n",
    "    #tpHead, tpBody, tstance_ids, tbody_ids, tnew_lab = retrieveHeadBody('competition_test',isSmall)\n",
    "    #createWord2VecDict(pHead, pBody, tpHead, tpBody)\n",
    "    return trainHead, trainBody, trainLab, valHead, valBody, valLab\n",
    "\n",
    "def prepare_test_data(isSmall):\n",
    "    pHead, pBody, stance_ids, body_ids, new_lab = retrieveHeadBody('competition_test',isSmall)\n",
    "    testBody = []\n",
    "    for fid in stance_ids:\n",
    "        testBody.append(pBody[body_ids.tolist().index(fid)])\n",
    "    \n",
    "    return pHead, testBody, new_lab\n",
    "\n",
    "def execGradientBoosting(isSmall):\n",
    "    # if vectorsExist() and not isSmall:\n",
    "    #     trainSentiment_feats, valSentiment_feats, testSentiment_feats, train_wvFeats, val_wvFeats, test_wvFeats, trainBase_feats, valBase_feats, testBase_feats, trainLabels, valLabels, testLabels = load_split_vectors()\n",
    "    # else:\n",
    "        #tHeadLine, tBody, tLabels, vHeadLine, vBody, vLabels = prepare_data_folds()\n",
    "        \n",
    "    trainHeadLine, trainBody, trainLabels, valHeadLine, valBody, valLabels = prepare_train_data(isSmall)\n",
    "    trainLabels = np.reshape(trainLabels,(len(trainLabels),1))\n",
    "    valLabels = np.reshape(valLabels,(len(valLabels),1))\n",
    "    \n",
    "    print('Data prepared and loaded')\n",
    "    trainSentiment_feats = sentiment_features(trainHeadLine, trainBody)\n",
    "    print('Train sentiment features generated....')\n",
    "    trainBase_feats = generate_features(trainHeadLine, trainBody)\n",
    "    print('Train baseline features generated....')\n",
    "    trainHead_wvfeats, trainBody_wvfeats = word2vec_features(trainHeadLine, trainBody)\n",
    "    train_wvFeats = []\n",
    "    for x in range(len(trainHead_wvfeats)):\n",
    "        train_wvFeats.append(np.concatenate((trainHead_wvfeats[x], trainBody_wvfeats[x])))\n",
    "    train_wvFeats = np.array(train_wvFeats)\n",
    "    print('Train word2vec features generated....')\n",
    "    \n",
    "    \n",
    "    valBase_feats = generate_features(valHeadLine, valBody)\n",
    "    print('Validation baseline features generated....')\n",
    "    valSentiment_feats = sentiment_features(valHeadLine, valBody)\n",
    "    print('Validation sentiment features generated....')\n",
    "    valHead_wvfeats, valBody_wvfeats = word2vec_features(valHeadLine, valBody)\n",
    "    val_wvFeats = []\n",
    "    for x in range(len(valHead_wvfeats)):\n",
    "        val_wvFeats.append(np.concatenate((valHead_wvfeats[x], valBody_wvfeats[x])))\n",
    "    val_wvFeats = np.array(val_wvFeats)\n",
    "    print('Validation word2vec features generated....')\n",
    "    \n",
    "    testHeadLine, testBody, testLabels = prepare_test_data(isSmall)\n",
    "    testHead_wvfeats, testBody_wvfeats = word2vec_features(testHeadLine, testBody)\n",
    "    print('Test word2vec features generated....')\n",
    "    testSentiment_feats = sentiment_features(testHeadLine, testBody)\n",
    "    print('Test sentiment features generated....')\n",
    "    testBase_feats = generate_features(testHeadLine, testBody)\n",
    "    print('Test baseline features generated....')\n",
    "    test_wvFeats = []\n",
    "    for x in range(len(testHead_wvfeats)):\n",
    "        test_wvFeats.append(np.concatenate((testHead_wvfeats[x], testBody_wvfeats[x])))\n",
    "        \n",
    "    \n",
    "    \n",
    "    train_X = np.hstack((train_wvFeats, trainSentiment_feats))\n",
    "    train_X = np.hstack((train_X, trainBase_feats))\n",
    "    val_X = np.hstack((val_wvFeats, valSentiment_feats))\n",
    "    val_X = np.hstack((val_X, valBase_feats))\n",
    "    \n",
    "    test_X = np.hstack((test_wvFeats, testSentiment_feats))\n",
    "    test_X = np.hstack((test_X, testBase_feats))\n",
    "    \n",
    "    clf = GradientBoostingClassifier(n_estimators=5, random_state=14128, verbose=True)\n",
    "    clf.fit(train_X, trainLabels)\n",
    "        \n",
    "    test_pred = clf.predict_proba(test_X)\n",
    "    test_p = np.argmax(test_pred, axis=1)\n",
    "    testPredictions = [round(value) for value in test_p]\n",
    "    val_pred = clf.predict_proba(val_X)\n",
    "    val_p = np.argmax(val_pred, axis=1)\n",
    "    valPredictions = [round(value) for value in val_p]\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(valLabels, valPredictions)\n",
    "    print(\"Accuracy over validation dataset: %.2f%%\" % (accuracy * 100.0))\n",
    "    print('Score over validation dataset set: ',fnc_score(valLabels, valPredictions))\n",
    "    \n",
    "    y_pred = pd.Series(valPredictions)\n",
    "    valLabels = np.array(valLabels)\n",
    "    valLabels = valLabels.reshape(valLabels.shape[0])\n",
    "    y_true = pd.Series(valLabels)\n",
    "    print('Confusion matrix over validation dataset: ')\n",
    "    print(pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(testLabels, testPredictions)\n",
    "    print(\"Accuracy over test dataset: %.2f%%\" % (accuracy * 100.0))\n",
    "    print('Score over test dataset set: ',fnc_score(testLabels, testPredictions))\n",
    "    \n",
    "    y_pred = pd.Series(testPredictions)\n",
    "    testLabels = np.array(testLabels)\n",
    "    testLabels = testLabels.reshape(testLabels.shape[0])\n",
    "    y_true = pd.Series(testLabels)\n",
    "    print('Confusion matrix over test data: ')\n",
    "    print(pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    implementDeepModel(trainSentiment_feats, valSentiment_feats, testSentiment_feats, train_wvFeats, val_wvFeats, test_wvFeats, trainBase_feats, valBase_feats, testBase_feats, trainLabels, valLabels, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared and loaded\n",
      "Train sentiment features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Train baseline features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Train word2vec features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Validation baseline features generated....\n",
      "Validation sentiment features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Validation word2vec features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Test word2vec features generated....\n",
      "Test sentiment features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Test baseline features generated....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1       27238.4524            1.37m\n",
      "         2       24250.3577            1.02m\n",
      "         3       22055.5658           40.93s\n",
      "         4       20337.5138           20.52s\n",
      "         5       18956.3492            0.00s\n",
      "Accuracy over validation dataset: 82.35%\n",
      "Score over validation dataset set:  62.93865747177576\n",
      "Confusion matrix over validation dataset: \n",
      "Predicted   1     2     3   All\n",
      "True                           \n",
      "0           3   182   518   703\n",
      "1          15    45   120   180\n",
      "2           0   937   842  1779\n",
      "3           0    54  7279  7333\n",
      "All        18  1218  8759  9995\n",
      "Accuracy over test dataset: 75.96%\n",
      "Score over test dataset set:  50.53749597682652\n",
      "Confusion matrix over test data: \n",
      "Predicted    1     2      3    All\n",
      "True                              \n",
      "0            2   259   1642   1903\n",
      "1            8    52    637    697\n",
      "2            7  1302   3155   4464\n",
      "3          266    90  17993  18349\n",
      "All        283  1703  23427  25413\n",
      "Train on 39977 samples, validate on 9995 samples\n",
      "Epoch 1/3\n",
      "39977/39977 [==============================] - 25s 613us/sample - loss: 0.7766 - accuracy: 0.7282 - val_loss: 0.6703 - val_accuracy: 0.7423\n",
      "Epoch 2/3\n",
      "39977/39977 [==============================] - 24s 595us/sample - loss: 0.5935 - accuracy: 0.7747 - val_loss: 0.4987 - val_accuracy: 0.8168\n",
      "Epoch 3/3\n",
      "39977/39977 [==============================] - 24s 591us/sample - loss: 0.4446 - accuracy: 0.8397 - val_loss: 0.3905 - val_accuracy: 0.8510\n",
      "Accuracy over validation set:  0.8510255127563782\n",
      "Accuracy over test set:  0.8340613072049738\n",
      "Fnc score over validation set:  70.08508981702909\n",
      "Fnc score over test set:  67.31466580838966\n",
      "Validation Set confusion matrix is: \n",
      "Predicted    0     2     3   All\n",
      "True                            \n",
      "0          191   241   271   703\n",
      "1           18    72    90   180\n",
      "2           60  1044   675  1779\n",
      "3            7    55  7271  7333\n",
      "All        276  1412  8307  9995\n",
      "Test Set confusion matrix is: \n",
      "Predicted     0     2      3    All\n",
      "True                               \n",
      "0           653   500    750   1903\n",
      "1            73   165    459    697\n",
      "2           402  2359   1703   4464\n",
      "3            24   141  18184  18349\n",
      "All        1152  3165  21096  25413\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/shubham/FinalProject/fnc/output_data/deepPred.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b13cdf69f799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexecGradientBoosting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-ad6252864acb>\u001b[0m in \u001b[0;36mexecGradientBoosting\u001b[0;34m(isSmall)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Confusion matrix over test data: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrownames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mimplementDeepModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainSentiment_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalSentiment_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestSentiment_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_wvFeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_wvFeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_wvFeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainBase_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalBase_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestBase_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/IIITH/Sem 4/Project/Project/src/fnc-comparision/deepModel.py\u001b[0m in \u001b[0;36mimplementDeepModel\u001b[0;34m(sentiment_train, sentiment_val, sentiment_test, wv_train, wv_val, wv_test, base_train, base_val, base_test, trainLabels, valLabels, testLabels)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;31m#pred_path = '/home/shubham/FinalProject/fnc/output_data/deepPred.pickle'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;31m#pickle_out = open(pred_path,\"wb\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;31m#pickle.dump(testPred, pickle_out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m#pickle_out.close()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/shubham/FinalProject/fnc/output_data/deepPred.pickle'"
     ]
    }
   ],
   "source": [
    "execGradientBoosting(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared and loaded\n",
      "Train sentiment features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Train baseline features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Train word2vec features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Validation baseline features generated....\n",
      "Validation sentiment features generated....\n",
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Validation word2vec features generated....\n"
     ]
    }
   ],
   "source": [
    "#Preparing Train Data\n",
    "isSmall = False\n",
    "trainHeadLine, trainBody, trainLabels, valHeadLine, valBody, valLabels = prepare_train_data(isSmall)\n",
    "trainLabels = np.reshape(trainLabels,(len(trainLabels),1))\n",
    "valLabels = np.reshape(valLabels,(len(valLabels),1))\n",
    "   \n",
    "print('Data prepared and loaded')\n",
    "trainSentiment_feats = sentiment_features(trainHeadLine, trainBody)\n",
    "print('Train sentiment features generated....')\n",
    "trainBase_feats = generate_features(trainHeadLine, trainBody)\n",
    "print('Train baseline features generated....')\n",
    "trainHead_wvfeats, trainBody_wvfeats = word2vec_features(trainHeadLine, trainBody)\n",
    "train_wvFeats = []\n",
    "for x in range(len(trainHead_wvfeats)):\n",
    "    train_wvFeats.append(np.concatenate((trainHead_wvfeats[x], trainBody_wvfeats[x])))\n",
    "train_wvFeats = np.array(train_wvFeats)\n",
    "print('Train word2vec features generated....')\n",
    "   \n",
    "    \n",
    "valBase_feats = generate_features(valHeadLine, valBody)\n",
    "print('Validation baseline features generated....')\n",
    "valSentiment_feats = sentiment_features(valHeadLine, valBody)\n",
    "print('Validation sentiment features generated....')\n",
    "valHead_wvfeats, valBody_wvfeats = word2vec_features(valHeadLine, valBody)\n",
    "val_wvFeats = []\n",
    "for x in range(len(valHead_wvfeats)):\n",
    "    val_wvFeats.append(np.concatenate((valHead_wvfeats[x], valBody_wvfeats[x])))\n",
    "val_wvFeats = np.array(val_wvFeats)\n",
    "print('Validation word2vec features generated....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1       27238.4524            1.37m\n",
      "         2       24250.3577            1.02m\n",
      "         3       22055.5658           41.05s\n",
      "         4       20337.5138           20.57s\n",
      "         5       18956.3492            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=5,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=14128, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=True,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training\n",
    "train_X = np.hstack((train_wvFeats, trainSentiment_feats))\n",
    "train_X = np.hstack((train_X, trainBase_feats))\n",
    "val_X = np.hstack((val_wvFeats, valSentiment_feats))\n",
    "val_X = np.hstack((val_X, valBase_feats))\n",
    "clf = GradientBoostingClassifier(n_estimators=5, random_state=14128, verbose=True)\n",
    "clf.fit(train_X, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#News In Indian Language\n",
    "Language = 'hi'\n",
    "HeadLine = 'लॉकडाउन 4 में ऑड-ईवन की तरह खुलें बाजार, दिल्ली सरकार ने केंद्र को भेजा प्रस्ताव'\n",
    "Body     = 'कोरोना वायरस संकट के बढ़ते मामलों के बीच दिल्ली सरकार की ओर से केंद्र सरकार को लॉकडाउन 4.0 के लिए प्रस्ताव भेज दिया गया है. दिल्ली के स्वास्थ्य मंत्री सत्येंद्र जैन ने शुक्रवार कोरोना वायरस संकट को लेकर मीडिया से बात की. उन्होंने इस दौरान कहा कि दिल्ली सरकार की ओर से केंद्र सरकार को लॉकडाउन 4.0 के लिए सुझाव भेजा जा चुका है. हम चाहते हैं कि बसों और मेट्रो को लिमिटेड तरीके से खोला जाए.सत्येंद्र जैन ने कहा कि हमने सरकार से पचास फीसदी तक मॉल खोलने को कहा है और बाजारों को ऑड-ईवन के आधार पर खोलने का सुझाव दिया है. इसके अलावा पब्लिक प्लेस में मास्क पहनना जरूरी कर देना चाहिए.'\n",
    "Actual_Lable = 'agree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline In English :  Lockdown open market like the odd-even 4, proposed government sent center\n",
      "NewsBody In English :  Corona virus crisis the central government on behalf of the government between the incidence has sent a proposal to the lockdown 4.0. Health Minister Satyendra Jain Delhi on Friday to talk to the media the corona virus crisis. He has sent you to lockdown 4.0 meantime said that the central government on behalf of the government. We want to have suggested that the buses and the subway opened to limited ways Jaa.satyendra Jain said that we have asked to open the mall to the government to fifty percent and markets open on the basis of the odd-even. Also should be required to wear masks in public place.\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "text = 'कंप्यूटर पर हिंदी में टायपिंग करना बहुत आसान बना दिया है'\n",
    "srcLang = 'hi'\n",
    "destLang   = 'en'\n",
    "Eng_Headline = translator.translate(HeadLine, dest=destLang , src=srcLang).text\n",
    "Eng_Body     = translator.translate(Body, dest=destLang , src=srcLang).text\n",
    "print('Headline In English : ',Eng_Headline)\n",
    "print('NewsBody In English : ',Eng_Body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "testHeadLine = [Eng_Headline]\n",
    "testBody     = [Eng_Body]\n",
    "testLabels   = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sanket/IIITH/Sem 4/Project/Project/output_data/word2vec.pickle\n",
      "Test word2vec features generated....\n",
      "Test sentiment features generated....\n",
      "word overlap fets generated...\n",
      "refuting fets generated...\n",
      "polarity fets generated...\n",
      "Test baseline features generated....\n"
     ]
    }
   ],
   "source": [
    "#Preparing Test Data\n",
    "testHead_wvfeats, testBody_wvfeats = word2vec_features(testHeadLine, testBody)\n",
    "print('Test word2vec features generated....')\n",
    "testSentiment_feats = sentiment_features(testHeadLine, testBody)\n",
    "print('Test sentiment features generated....')\n",
    "testBase_feats = generate_features(testHeadLine, testBody)\n",
    "print('Test baseline features generated....')\n",
    "test_wvFeats = []\n",
    "for x in range(len(testHead_wvfeats)):\n",
    "    test_wvFeats.append(np.concatenate((testHead_wvfeats[x], testBody_wvfeats[x])))\n",
    "    \n",
    "test_X = np.hstack((test_wvFeats, testSentiment_feats))\n",
    "test_X = np.hstack((test_X, testBase_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "test_pred = clf.predict_proba(test_X)\n",
    "test_p = np.argmax(test_pred, axis=1)\n",
    "testPredictions = [round(value) for value in test_p]\n",
    "val_pred = clf.predict_proba(val_X)\n",
    "val_p = np.argmax(val_pred, axis=1)\n",
    "valPredictions = [round(value) for value in val_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_p[0]\n",
    "if res == 3:\n",
    "    print('unrelated')\n",
    "if res == 0:\n",
    "    print('agree')\n",
    "if res == 2:\n",
    "    print('discuss')\n",
    "if res == 1:\n",
    "    print('disagree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
