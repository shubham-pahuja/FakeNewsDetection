# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XKkRLvW2KSaJ_8aENgtpt1QcuIWs9b6j
"""

import nltk

train_stance_url='https://raw.githubusercontent.com/FakeNewsChallenge/fnc-1/master/train_stances.csv'
train_bodies_url='https://raw.githubusercontent.com/FakeNewsChallenge/fnc-1/master/train_bodies.csv'

import pandas as pd
import numpy as np
stances_train = pd.read_csv(train_stance_url)
body_train = pd.read_csv(train_bodies_url)
train = pd.merge(stances_train, body_train, how='left', on='Body ID')

train[0:2]

train.shape

dic={'agree':0,'disagree':1,'discuss':2,'unrelated':3}
train['Stance']=train['Stance'].map(dic)

train[0:1]

nltk.download('stopwords')

english_stemmer = nltk.stem.SnowballStemmer('english')
token_pattern = r"(?u)\b\w\w+\b"
stopwords = set(nltk.corpus.stopwords.words('english'))

def stem_tokens(tokens, stemmer):
    stemmed = []
    for token in tokens:
        stemmed.append(stemmer.stem(token))
    return stemmed

import re
def preprocess_data(line,
                    token_pattern=token_pattern,
                    exclude_stopword=True,
                    stem=True):
    token_pattern = re.compile(token_pattern, flags = re.UNICODE)
    tokens = [x.lower() for x in token_pattern.findall(line)]
    tokens_stemmed = tokens
    if stem:
#         pass
        tokens_stemmed = stem_tokens(tokens, english_stemmer)
    if exclude_stopword:
#         pass
        tokens_stemmed = [x for x in tokens_stemmed if x not in stopwords]

    return tokens_stemmed

preprocess_data('She is running. He ran away')
# train[0:5]

# preprocess_data(train['Headline'])

"""Concatinating headline and body strings"""

train['combined']= train['Headline'] +" "+ train['articleBody']

train['combined'][0]

train['Headline'][0]

"""**Preprocessing** the combined string = Stemming + Stop words removal

Generating **unigrams**= ['this , 'is' , 'a' ,'cat']
"""

unigram=[]
for i in train['combined']:
  unigram.append(preprocess_data(i))

head_unigram=[]
for i in train['Headline']:
  head_unigram.append(preprocess_data(i))

body_unigram=[]
for i in train['articleBody']:
  body_unigram.append(preprocess_data(i))

unigram_np = np.array(unigram)
unigram_np.shape

"""**Making strings from unigrams**"""

unigram_comb=[]
for count,i in enumerate(unigram_np):
  unigram_comb.append(' '.join(unigram_np[count]))

unigram_comb_head=[]
for count,i in enumerate(head_unigram):
  unigram_comb_head.append(' '.join(head_unigram[count]))

unigram_comb_body=[]
for count,i in enumerate(body_unigram):
  unigram_comb_body.append(' '.join(body_unigram[count]))

len(unigram_comb)

len(unigram_comb[0])

len(unigram)

train['headline_unigram']=np.array(unigram_comb_head)
train['combined_unigram']=np.array(unigram_comb)
train['body_unigram']=np.array(unigram_comb_body)

del(unigram)
del(head_unigram)
del(body_unigram)
del(unigram_comb)
del(unigram_comb_head)
del(unigram_comb_body)
del(stances_train)
del(body_train)
del(unigram_np)
import gc
collected= gc.collect()
print("garbage collected= ",collected)

vocab_size=4000
from sklearn.feature_extraction.text import TfidfVectorizer
vec = TfidfVectorizer(ngram_range=(1,1), max_df=0.8, min_df=2,max_features=vocab_size)
vec.fit(train["combined_unigram"]) # Tf-idf calculated on the combined training + test set
vocabulary = vec.vocabulary_
del(vec)

vecH = TfidfVectorizer(ngram_range=(1,1), max_df=0.8, min_df=2, vocabulary=vocabulary)
xHeadlineTfidf = vecH.fit_transform(train['headline_unigram']) # use ' '.join(Headline_unigram) instead of Headline since the former is already stemmed
print ('xHeadlineTfidf.shape:')
print (xHeadlineTfidf.shape)
del(vecH)

len(vocabulary)

xHeadlineTfidf[0]

vecB = TfidfVectorizer(ngram_range=(1, 1), max_df=0.8, min_df=2, vocabulary=vocabulary,max_features=vocab_size)
xBodyTfidf = vecB.fit_transform(train['body_unigram'])
print ('xBodyTfidf.shape:')
print (xBodyTfidf.shape)
del(vecB)
del(vocabulary)

train[0:1]

print(xHeadlineTfidf.shape)
print(xBodyTfidf.shape)

type(xHeadlineTfidf)

from scipy.sparse import csr_matrix
xheadline= xHeadlineTfidf.toarray()
xbody=xBodyTfidf.toarray()
del(xHeadlineTfidf)
del(xBodyTfidf)

print(xheadline.shape)
print("body shape")
print(xbody.shape)

x= np.hstack((xheadline,xbody))
print(x.shape)

print(train.columns)
# train['xHeadlineTfidf']=xHeadlineTfidf
# train['xBodyTfidf']=xBodyTfidf
# train['combinedTfidf']=x

y= train['Stance']
print(x.shape)
print(y.shape)
del(train)
del(xheadline)
del(xbody)

import gc
collected= gc.collect()
print("garbage collected= ",collected)

from numpy import loadtxt
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

f=open('tfidf_x.pickle','rb')
x= pk.load(f)
f.close()

seed = 7
test_size = 0.33
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=seed)

import pickle as pk
f=open('tfidf_x.pickle','wb')
pk.dump(x,f)
f.close()
del(x)

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)  

# get the folder id where you want to save your file
file = drive.CreateFile()
file.SetContentFile('tfidf_x.pickle')
file.Upload()
# del(x)
import gc
collected= gc.collect()
print("garbage collected= ",collected)

!pip install pydrive

# seed = 7
# test_size = 0.33
# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)
# fit model no training data

model = XGBClassifier()
model.fit(X_train, y_train)
print("model fit done")
# make predictions for test data
y_pred = model.predict(X_test)
print("predictions obtained")
predictions = [round(value) for value in y_pred]
# evaluate predictions
print("calculating accuracy")
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

from google.colab import drive
drive.mount('/content/drive')

import os
os.getcwd()

os.chdir('drive/My Drive')

os.listdir()

from numpy import loadtxt
# from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pickle as pk
f=open('tfidf_x.pickle','rb')
x= pk.load(f)
f.close()
y=train['Stance']
seed = 7
test_size = 0.33
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=seed)

# y=train['Stance']
del(train)
del(x)
del(f)
import gc
collected= gc.collect()
print("garbage collected= ",collected)

import gc
collected= gc.collect()
print("garbage collected= ",collected)

from sklearn.ensemble import GradientBoostingClassifier
clf = GradientBoostingClassifier(n_estimators=200, random_state=14128, verbose=True)
clf.fit(X_train, y_train)
from sklearn.metrics import accuracy_score

#model.fit(trainSentiment_feats, trainLabels)
#model.fit(trainSentiment_feats, trainLabels)
# make predictions for test data

val_pred = clf.predict(X_test)
y_predict = [round(value) for value in val_pred]
# evaluate predictions
accuracy = accuracy_score(y_test, y_predict)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
# print('Score over validation set: ',fnc_score(valLabels, valPredictions))

def score(gold_lab, test_lab):
    score = 0.0
    for (g,t) in zip(gold_lab, test_lab):
        if g == t:
            score+=0.25
            if g != 3:
                score+=0.5
        if g in [0,1,2] and t in [0,1,2]:
            score+=0.25
    
    return score

def fnc_score(actual, predicted):
    actual_score = score(actual, actual)
    calc_score = score(actual, predicted)
    return (calc_score*100)/actual_score

LABELS = ['agree', 'disagree', 'discuss', 'unrelated']
LABELS_RELATED = ['unrelated','related']
RELATED = LABELS[0:3]

def score_submission(gold_labels, test_labels):
    score = 0.0
    cm = [[0, 0, 0, 0],
          [0, 0, 0, 0],
          [0, 0, 0, 0],
          [0, 0, 0, 0]]

    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):
        g_stance, t_stance = g, t
        if g_stance == t_stance:
            score += 0.25
            if g_stance != 'unrelated':
                score += 0.50
        if g_stance in RELATED and t_stance in RELATED:
            score += 0.25

        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1

    return score, cm


def print_confusion_matrix(cm):
    lines = []
    header = "|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|".format('', *LABELS)
    line_len = len(header)
    lines.append("-"*line_len)
    lines.append(header)
    lines.append("-"*line_len)

    hit = 0
    total = 0
    for i, row in enumerate(cm):
        hit += row[i]
        total += sum(row)
        lines.append("|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|".format(LABELS[i],
                                                                   *row))
        lines.append("-"*line_len)
    print('\n'.join(lines))


def report_score(actual,predicted):
    score,cm = score_submission(actual,predicted)
    best_score, _ = score_submission(actual,actual)

    print_confusion_matrix(cm)
    print("Score: " +str(score) + " out of " + str(best_score) + "\t("+str(score*100/best_score) + "%)")
    return score*100/best_score

print('Score over validation set: ',fnc_score(y_test, y_predict))

import pickle as pk
f=open('tfidf_ypredict.pickle','wb')
pk.dump(y_predict,f)
f.close()

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)  

# get the folder id where you want to save your file
file = drive.CreateFile()
file.SetContentFile('tfidf_ypredict.pickle')
file.Upload()
# del(x)
import gc
collected= gc.collect()
print("garbage collected= ",collected)

!pip install pydrive

